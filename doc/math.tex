\documentclass[10pt]{article}
\usepackage[margin=0.7in]{geometry}
\usepackage{amsmath, amssymb, amsfonts, bbm, enumitem, enotez, tikz, pgfplots, cancel, graphicx,
caption, algorithm, algorithmic, hyperref}
\usetikzlibrary{matrix, arrows.meta, positioning, angles}
\hypersetup{colorlinks=true, urlcolor=blue, linkcolor=violet}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\theendnote}{\alph{endnote}}

\begin{document}\allowdisplaybreaks

\title{Spectral Derivatives}
\author{Pavel Komarov}
\date{January 8, 2025}
\maketitle

One of the happiest accidents in all math is the ease of taking derivatives in the Fourier (i.e. the \textit{frequency}) domain. But in order to exploit this extraordinary fact without serious artefacting, and in order to be able to use a computer, we need quite a bit of extra knowledge and care.

This document sets out the math behind the \texttt{spectral-derivatives} package, all the way down to the bones, as much as I can manage. I try to get in to the real \textit{whys} behind what we're doing here, touching on fundamental signal processing and calculus concepts as necessary, and building upwards to more general cases.

% Add an enumerate here to outline things? Or a table of contents?
\tableofcontents
\pagebreak

\section{Bases}

A \textit{basis} is a set of ``orthogonal" functions, call them $\{\xi_k\}$, that can be summed together in various quantities to produce other functions. Othogonal means that if we take the ``inner product" of one funtion from the set with itself, we get back 1, and if we take the inner product of a function with a different memnber of the set, we get back 0. In this sense the members of the basis set are independent of one another, just like perpendicular directions on a graph.

The inner product between two functions $f$ and $g$ is a generalization of the inner product between vectors, where instead of summing over a finite number of discrete entries, we integrate over infinitely many infinitesimally-separated points in the domain. We define it as:

$$ \langle f,g \rangle = \int\limits_{a}^{b} \overline{f(x)} g(x) dx $$

where the overbar $\overline{\cdot}$ denotes a complex conjugate.\newline

The inner product is symmetrical, so

$$ \langle f,g \rangle = \langle g,f \rangle = \int\limits_{a}^{b} f(x) \overline{g(x)} dx $$

Note that if we set $a$ and $b$ at $\pm \infty$, this integral could diverge. If it doesn't diverge with infinite bounds, we say the argument is \href{https://mathworld.wolfram.com/LebesgueIntegrable.html}{``Lebesgue integrable"}\cite{lebesgue}. Some of what we'll do only makes sense for this class of functions, so be aware.

\subsection{The Fourier Basis}

The most famous basis is the \textit{Fourier} basis\endnote{There's a great passage in Richard Hamming's book \textit{The Art of Doing Science and Engineering}\cite{hamming} where he wonders why we use the Fourier basis so much:

\begin{quotation}
``It soon became clear to me digital filter theory was dominated by Fourier series, about which theoretically I had learned in college, and actually I had had a lot of further education during the signal processing I had done for John Tukey, who was a professor from Princeton, a genius, and a one or two day a week employee of Bell Telephone Laboratories. For about ten years I was his computing arm much of the time.

Being a mathematician I knew, as all of you do, that any complete set of functions will do about as good as any other set at representing arbitrary functions. Why, then, the exclusive use of the Fourier series? I asked various electrical engineers and got no satisfactory answers. One engineer said alternating currents were sinusoidal, hence we used sinusoids, to which I replied it made no sense to me. So much for the usual residual education of the typical electrical engineer after they have left school!

So I had to think of basics, just as I told you I had done when using an error-detecting computer. What is really going on? I suppose many of you know what we want is a time-invariant representation of signals, since there is usually no natural origin of time. Hence we are led to the trigonometric functions (the eigenfunctions of translation), in the form of both Fourier series and Fourier integrals, as the tool for representing things.

Second, linear systems, which is what we want at this stage, also have the same eigenfunctionsâ€”the complex exponentials which are equivalent to the real trigonometric functions. Hence a simple rule: if you have either a time-invariant system or a linear system, then you should use the complex exponentials.

On further digging into the matter I found yet a third reason for using them in the field of digital filters. There is a theorem, often called Nyquist's sampling theorem (though it was known long before and even published by Whittaker, in a form you can hardly realize what it is saying, even when you know Nyquist's theorem), which says that if you have a band-limited signal and sample at equal spaces at a rate of at least two in the highest frequency, then the original signal can be reconstructed from the samples. Hence the sampling process loses no information when we replace the continuous signal with the equally spaced samples, provided the samples cover the whole real line. The sampling rate is often known as the Nyquist rate after Harry Nyquist, also of servo stability fame, as well as other things [also reputed to have been just a really great guy who often had productive lunches with his colleagues, giving them feedback and asking questions that brought out the best in them]. If you sample a non-band-limited function, then the higher frequencies are ``aliased" into lower ones, a word devised by Tukey to describe the fact that a single high frequency will appear later as a single low frequency in the Nyquist band. The same is not true for any other set of functions, say powers of $t$. Under equally spaced sampling and reconstruction a single high power of t will go into a polynomial (many terms) of lower powers of $t$.

Thus there are three good reasons for the Fourier functions: (1) time invariance, (2) linearity, and (3) the reconstruction of the original function from the equally spaced samples is simple and easy to understand.

Therefore we are going to analyze the signals in terms of the Fourier functions, and I need not discuss with electrical engineers why we usually use the complex exponents as the frequencies instead of the real trigonometric functions. [It's down to convenience, really.] We have a linear operation, and when we put a signal (a stream of numbers) into the filter, then out comes another stream of numbers. It is natural, if not from your linear algebra course then from other things such as a course in differential equations, to ask what functions go in and come out exactly the same except for scale. Well, as noted above, they are the complex exponentials; they are the eigenfunctions of linear, time-invariant, equally spaced sampled systems.

Lo and behold, the famous transfer function [contains] exactly the eigenvalues of the corresponding eigenfunctions! Upon asking various electrical engineers what the transfer function was, no one has ever told me that! Yes, when pointed out to them that it is the same idea they have to agree, but the fact it is the same idea never seemed to have crossed their minds! The same, simple idea, in two or more different disguises in their minds, and they knew of no connection between them! Get down to the basics every time!"\end{quotation}}, which is the set of complex exponentials

\begin{equation}\label{euler}
e^{j \omega} = \cos(\omega) + j \sin(\omega)
\end{equation}

where I'm using $j$ to represent the imaginary unit, because I'm from Electrical Engineering, and because Python uses \texttt{j}.\newline

Why this identity is true isn't obvious at first but can be seen by \href{https://math.stackexchange.com/a/492165/278341}{Taylor Expanding}\cite{taylor} the exponential function and trigonometric functions:

$$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + ... = \sum_{n=0}^{\infty} \frac{x^n}{n!}$$

So

$$ e^{j \omega} = 1 + j \omega + \frac{(j \omega)^2}{2!} + \frac{(j \omega)^3}{3!} + ... = 1 + j \omega - \frac{\omega^2}{2!} - j \frac{\omega^3}{3!} + \frac{\omega^4}{4!} - ... $$

$$ \sin(\omega) = \omega - \frac{\omega^3}{3!} + \frac{\omega^5}{5!} - \frac{\omega^7}{7!} + ... $$

$$ \cos(\omega) = 1 - \frac{\omega^2}{2!} + \frac{\omega^4}{4!} - \frac{\omega^6}{6!} + ... $$

Notice all of the even-power terms appear with alternating sign as in the cosine expansion, and the odd-power terms appear with alternating sign as in the sine expansion, but with an extra $j$ multiplied in.

The presence of complex numbers to make this work can be confusing at first, but don't be scared! All we're really doing is using a compressed representation of a sine plus a cosine, where the real and imaginary parts (orthogonal in the complex plane, and therefore independent and non-interfering) allow us to describe the contributions of sine and cosine simultaneously. In fact, \href{https://math.stackexchange.com/a/1293127/278341}{Joseph Fourier originally used only real trigonometric functions}\cite{complex}, and it wasn't until later someone decided it would be easier to work with complex exponentials. Later (\autoref{fourier}) we'll see that for real signals all the complex numbers cancel, leaving only a \textit{real} sine and \textit{real} cosine, which when added together make \textit{a single, phase-shifted sinusoid!} So think of $e^{j \omega}$ as oscillations at a particular frequency, $\omega$.

If we inner product mismatched wiggles, they misalign and integrate to 0, but if we inner product matched wiggles, they align, multiply to 1 because of the complex conjugate, and integrate to $2\pi$ over a period.

\section{Transforms}

I can use a basis to ``transform" a function, meaning I take the function's inner product with each of the basis functions to produce numbers:

$$ \langle f,\xi_k \rangle = \int\limits_{a}^{b} f(x) \overline{\xi_k(x)} dx = \text{a constant coefficient}, c_k $$

These numbers descibe \textit{how much} of each basis function $\xi_k$ is present in the signal $f$ on a domain between $a$ and $b$. If I do this for all the $\{\xi_k\}$, I essentially get a \textit{recipe}, which says ``I need this much of $\xi_0$ and that much of $\xi_1$ and howevermuch of $\xi_2$ ... added together to reproduce the original signal."

$$ f(x) = \sum_{k=0}^{M-1} c_k \xi_k(x), \quad x \in [a, b] $$

where $M$ is the number of basis functions I'm using in my reconstruction.\newline

The set of numbers $c_k$ is now \textit{an alternative representation} of the original function. In some sense it's equally descriptive, so long as we know which basis we're using to reconstruct. We've \textit{transformed} the function to a set of numbers which now live in a different domain.

Beware that the terminology ``transform" and ``domain" is not always used to describe transforming a continuous function to a discrete set of $c_k$ like this, because ``domain" technically refers to a ``connected" set, not just a collection of $k$ things. However, it is possible for members of the basis set to be related through a continuous parameter which in some sense makes the set dense, having infinitely many members which are infinitesimally close together. This is the case for the Fourier basis, where we choose $\omega \in \mathbb{R}$, and hence $\omega$ really can become a new domain.

\subsection{The Fourier Transform}\label{fourier}

Using Fourier's original real-sinusoid-based formulation, we can write the reconstruction expression as\footnote{It's worth considering how weird it is this is true. In fact, it's so weird that Joseph Lagrange publicly declared Fourier was wrong at a meeting of the Paris Academy in 1807!\cite{michigan} It's valuable to ask \href{https://math.stackexchange.com/questions/1105265/why-do-fourier-series-work}{why this works}\cite{why} and sift through some analysis.}:

$$ f(x) = a_0 + \sum_{k=1}^{\infty} (a_k \cos(k \omega_0 x) + b_k \sin(k \omega_0 x))$$

where
\begin{itemize}[noitemsep, topsep=0pt, after=\newline]
	\item $f$ is periodic with fundamental frequency $\omega_0$, so the $k^{th}$ frequency becomes $k \cdot \omega_0$
	\item $a_k$ and $b_k$ are coefficients describing how much cosine and sine to add in, respectively
	\item $k$ goes up to $\infty$ because we need an infinite number of ever-higher-frequency sinusoids to reconstruct the function with perfect fidelity
\end{itemize}

Let's now use $\cos(x) = \frac{e^{jx} + e^{-jx}}{2}$ and $\sin(x) = \frac{e^{jx} - e^{-jx}}{2j}$, which can be verified by manipulating Euler's formula, \autoref{euler}.

\begin{align*}
f(x) &= a_0 + \sum_{k=1}^{\infty} (a_k \frac{e^{j k \omega_0 x} + e^{-j k \omega_0 x}}{2} + b_k \frac{e^{j k \omega_0 x} - e^{-j k \omega_0 x}}{2j}) \\
&= a_0 + \sum_{k = -\infty}^{-1} (\frac{a_{-k}}{2} - \frac{b_{-k}}{2j}) e^{j k \omega_0 x} + \sum_{k = 1}^{\infty} (\frac{a_k}{2} + \frac{b_k}{2j}) e^{j k \omega_0 x} = \sum_{k = -\infty}^{\infty} c_k e^{j k \omega_0 x}
\end{align*}

So if we choose $c_0 = a_0$ and $c_k = \overline{c_{-k}} = \frac{a_k}{2} + \frac{b_k}{2j}$, then the complex exponential formulation is \href{http://lpsa.swarthmore.edu/Fourier/Series/DerFS.html}{exactly equivalent to the trigonometric formulation}\cite{swarthmore}. That is, we can choose \textit{complex} $c_k$ such that when multiplied by \textit{complex} exponentials, we get back only \textit{real} signal! Essentially, the relative balance of real and complex in $c_k$ affects how much cosine and sine are present at the $k^{th}$ frequency, \href{https://dsego.github.io/demystifying-fourier/}{thereby accomplishing a phase shift}\cite{sego}. Without accounting for phase shifts, we would only be able to model \textit{symmetric} signals!

If instead of a fundamental frequency $\omega_0 = \frac{2\pi}{T}$, where $T$ is a period of repetition, the signal contains dense frequencies (because it has no repetition, $T \rightarrow \infty$, $\omega_0 \rightarrow 0$), and if we care about a domain of the entire set of $\mathbb{R}$, then it makes more sense to express the transformed coefficients as a function in $\omega$ and to make both our inner product and reconstruction expression integrals from $-\infty$ to $+\infty$:


\begin{equation}\label{pair}
\begin{aligned}
\hat{f}(\omega) &= \int\limits_{-\infty}^{\infty} f(x) e^{-j \omega x} dx = \mathcal{F}\{f(x)\} \\
f(x) &= \frac{1}{2\pi} \int\limits_{-\infty}^{\infty} \hat{f}(\omega) e^{j \omega x} d \omega = \mathcal{F}^{-1}\{\hat{f}(\omega)\}
\end{aligned}
\end{equation}


where the hat $\hat{\cdot}$ represents a function in the Fourier domain, and the $\frac{1}{2\pi}$ is a scaling factor that corrects for the fact the inner product of a Fourier basis function with itself integrates to $2\pi$ over a period instead of to $1$ as we need for orthonormality.

Just like the $c_k$, $\hat{f}(\omega)$ can be complex, but if the original $f(x)$ is real, then $\hat{f}$'s complexity will perfectly interact with the complex exponentials to produce only a real function in the reconstruction. 

\subsection{A Whole Family}\label{family}

Part of what makes Fourier transforms confusing is the proliferation of different variants for different situations, so it's worth \href{https://medium.com/sho-jp/fourier-transform-101-part-4-discrete-fourier-transform-8fc3fbb763f3 }{categorizing them}.\cite{medium}. First off, are we dealing with a periodic signal (which has an $\omega_0$) or an aperiodic signal (which doesn't)? And second, are we dealing with a continuous function or discrete samples?

\begin{center}
\begin{tikzpicture}
\matrix (m) [
	matrix of nodes,
	nodes={draw, align=center, text height=1.5ex, text depth=.25ex, anchor=center}, column sep=0pt, row sep=0pt,
	cells={nodes={draw, minimum width=5cm, minimum height=4cm, outer sep=0pt}}
] at (0, 0) {
	\node (A) {}; & \node (B) {}; \\
	\node (C) {}; & \node (D) {}; \\
};

\node[anchor=south] at (A.north) {Periodic};
\node[anchor=south] at (B.north) {Aperiodic};
\node[anchor=east, rotate=90, yshift=2mm, xshift=10mm] at (A.west) {Continuous};
\node[anchor=east, rotate=90, yshift=2mm, xshift=7mm] at (C.west) {Discrete};

\node at (-3.25,2.5) (x_t_periodic) {$x(t)$}; 
\node at (-2,1.5) (X_ejw) {$X(e^{j\omega}$)};
\node at (3,3) (x_t_aperiodic) {$x(t)$}; 
\node at (3,1.5) (X_jw) {$X(j\omega$)};
\node at (-3, -1.5) (x_n_periodic) {$x[n]$};
\node at (-3, -3) (X_k) {$X[k]$};
\node at (2, -1.5) (x_n_aperiodic) {$x[n]$};
\node at (3.25, -2.5) (c_k) {$c_k$};

\draw[->, bend right=30] (X_ejw) to node[midway] {DTFT$^{-1}$} (x_n_aperiodic);
\draw[->, bend right=30] (x_n_aperiodic) to node[midway] {DTFT} (X_ejw);
\draw[->, bend left=45] (c_k) to node[midway] {FS$^{-1}$} (x_t_periodic);
\draw[->, bend left=45] (x_t_periodic) to node[midway] {FS} (c_k);
\draw[->, bend left=90] (X_k) to node[midway] {DFT$^{-1}$} (x_n_periodic);
\draw[->, bend left=90] (x_n_periodic) to node[midway] {DFT} (X_k);
\draw[->, bend left=90] (x_t_aperiodic) to node[midway] {FT} (X_jw);
\draw[->, bend left=90] (X_jw) to node[midway] {FT$^{-1}$} (x_t_aperiodic);
\end{tikzpicture}
\end{center}

Note that, following a more signal-processing-y convention\cite{oppenheim}, the function we're transforming is now called $x$, and the independent variable, since it can no longer be $x$, is named $t$. For discrete signals, we use independent variable $n$ in square brackets.

Here FS stands for ``Fourier Series", which is the first situation covered above. FT stands for ``Fourier Transform", which is given by the integral pair, \autoref{pair}. But these are not the only possibilities! DTFT stands for ``Discrete Time Fourier Transform", where the signal we want to analyze is discrete but the transform is continuous. And finally DFT stands for ``Discrete Fourier Transform", not to be confused with the DTFT, which we use when \textit{both} the original and transformed signals are sampled.

\textit{All} of these can be considered Fourier transforms, but often when people talk about \textit{the} canonical ``Fourier capital-T Transform", they are referring to the continuous, aperiodic case in the upper righthand cell.

The notation of all these different functions and transforms is easy to mix up and made all the more confusing by the reuse of symbols. But it's important to keep straight which situation we're in. \href{https://www.youtube.com/watch?v=6ITWKtTYlEI&t=69s}{I can only apologize.} For more on all these, see \cite{oppenheim}.

\section{Taking Derivatives in the Fourier Domain}\label{derivative}

Let's \href{https://www.youtube.com/watch?v=d5d0ORQHNYs}{take a Fourier transform of the derivative of a function}\cite{brunton}:

$$\mathcal{F}\{\frac{d}{dx} f(x)\} = \int\limits_{-\infty}^{\infty} \underbrace{\frac{df}{dx}}_{dv} \underbrace{e^{-j \omega x}}_{u} dx = \underbrace{f(x) e^{-j \omega x} \Big|_{-\infty}^{\infty}}_{\parbox{25mm}{\footnotesize \centering 0 for Lebesgue-integrable functions}} - \int\limits_{-\infty}^{\infty} f(x) (-j \omega) e^{-j \omega x} dx = j \omega \cdot \hat{f}(\omega)$$

We can use the inverse transform equation to see the same thing:

$$\frac{d}{dx} f(x) = \frac{d}{dx} \frac{1}{2\pi} \int\limits_{-\infty}^{\infty} \hat{f}(\omega) e^{j \omega x} d \omega = \frac{1}{2\pi} \int\limits_{-\infty}^{\infty} \hat{f}(\omega) \frac{d}{dx} e^{j \omega x} d \omega = \mathcal{F}^{-1}\{j \omega \cdot \hat{f}(\omega)\}$$

So a derivative in the $x$ domain can be accomplished by a \textit{multiplication} in the frequency domain. We can raise to higher derivatives simply by multiplying by $j \omega$ more times.

This is great because taking derivatives in the spatial domain is actually pretty hard, especially if we're working with discrete samples of a signal, whereas taking the derivative this way in the frequency domain, the \textit{spectral derivative}, gives us much better fidelity.\cite{kutz} The cost is that we have to do a Fourier transform and inverse Fourier transform to sandwich the actual differentiation, but there is an $O(N \log N)$ algorithm to accompish the DFT (\autoref{family}, \autoref{dft}) for discrete signals called the Cooley-Tukey algorithm, also known as the Fast Fourier Transform (FFT)\cite{kutz}.

\subsection{Taking Derivatives in the Discrete Case}

Because we're going to want to use a computer, and a computer can only operate on discrete representations, we really need to talk about the DFT and what it means to take a derivative in this discrete paradigm. It has a connection to the above continuous case but is far more subtle, worth going in to \textit{\href{https://www.youtube.com/watch?v=v7l3Q11DBq0&t=1299s}{at some length}}.

\subsubsection{The DFT Pair}

\begin{equation}\label{dft}
\begin{aligned}
\text{DFT: \ \ } Y_k &= \sum_{n=0}^{M-1} y_n e^{-j \frac{2\pi}{M} n k} \\
\text{DFT} ^{-1} \text{: \ \ } y_n &= \frac{1}{M} \sum_{k=0}^{M-1} Y_k e^{j \frac{2\pi}{M} n k}
\end{aligned}
\end{equation}

where
\begin{itemize}[noitemsep, topsep=0pt, after=\newline]
	\item $n$ iterates samples in the original domain (often spatial)
	\item $k$ iterates samples in the frequency domain (wavenumbers)
	\item $M$ is the number of samples in the signal, often given as $N$ by \href{https://numpy.org/doc/2.1/reference/routines.fft.html}{other sources}\cite{numpy}, but I'll use $N$ for something else later and want to be consistent
	\item $y$ denotes the signal in its original domain
	\item $Y$ denotes the signal in the frequency domain
\end{itemize}

To see where this comes from, see \href{https://atmos.washington.edu/~breth/classes/AM585/lect/DFT_FS_585_notes.pdf}{Bretherton}\cite{bretherton}, among others\cite{oppenheim}.\newline

For simplicity, we can collect $\frac{2\pi}{M}n$ as a single term, $\theta_n \in [0, 2\pi)$, or $\frac{2\pi}{M}k$ as a single term, $\theta_k$. We then get $y_n = y(\theta_n)$ and $Y_k = Y(\theta_k)$. This may help highlight the fact the \href{https://dsp.stackexchange.com/a/18931/40873}{original signal and transformed signal live on a domain which maps to the unit circle}\cite{bristow} (hence periodicity and aliasing) and are being sampled at equally-spaced angles.

\subsubsection{Interpolation}

I now quote \href{https://math.mit.edu/~stevenj/fft-deriv.pdf}{Steven Johnson}\cite{johnson}, with some of my own symbols and notation sprinkled in:

\begin{quotation}
``In order to compute derivatives like $y'(\theta)$, we need to do more than express $y_n$. We need to use the DFT$^{-1}$ expression to define a continuous interpolation between the samples $y_n$---this is called \textit{trigonometric interpolation}---and then differentiate this interpolation. At first glance, interpolating seems very straightforward: one simply evaluates the DFT$^{-1}$ expression at non-integer $n \in \mathbb{R}$. This indeed defines \textit{an} interpolation, but it is not the \textit{only} interpolation, nor is it the \textit{best} interpolation for this purpose. The reason there is more than one interpolation is due to \textit{aliasing}: any term $e^{+j \theta_n k} Y_k$ in the DFT$^{-1}$ can be replaced by $e^{+j \theta_n (k + mM)} Y_k$ for any integer $m$ and still give the \textit{same} samples $y_n$, since $e^{j \frac{2\pi}{M} nmM} = e^{j2\pi nm} = 1$ for any integers $m$ and $n$. Essentially, adding the $mM$ term to $k$ means that the interpolated function $y(\theta)$ just oscillates $m$ extra times between the sample points, which has no effect on $y_n$ but has a huge effect on derivatives. To resolve this ambiguity, one imposes additional criteria---e.g. a bandlimited spectrum and/or minimizing some derivative of the interpolated $y(\theta)$"
\end{quotation}

We can now posit a slightly more general formula for the underlying continuous, periodic (over interval length M) signal:

$$ y(\theta) = \frac{1}{M} \sum_{k=0}^{M-1} Y_k e^{j \theta (k + m_k M)}, \quad m_k \in \mathbb{Z} $$

\begin{quotation}
``In order to uniquely determine the $m_k$, a useful criterion is that we wish to \textit{oscillate as little as possible} between the sample points $y_n$. One way to express this idea is to assume that $y(\theta)$ is \textit{bandlimited} to frequences $|k + m_k M| \leq \frac{M}{2}$. Another approach, that gives the same result ... is to \textit{minimize the mean-square slope}"\footnote{It's due to this ambiguity and constraint that spectral methods are only suitable for smooth functions!}
\end{quotation}

\begin{align*}
\frac{1}{2\pi} \int\limits_{0}^{2\pi} |y'(\theta)|^2 d\theta &= \frac{1}{2\pi} \int\limits_{0}^{2\pi} \Big|\frac{1}{M} \sum_{k=0}^{M-1} j(k + m_k M) Y_k e^{j \theta (k + m_k M)} \Big|^2 d\theta \\
&= \frac{1}{2\pi M^2} \int\limits_{0}^{2\pi} \Big( \sum_{k=0}^{M-1} j(k + m_k M) Y_k e^{j \theta (k + m_k M)} \Big) \overline{\Big( \sum_{k=0}^{M-1} j(k + m_k M) Y_k e^{j \theta (k + m_k M)} \Big)} d\theta \\
&= \frac{1}{2\pi M^2} \int\limits_{0}^{2\pi} \sum_{k=0}^{M-1} \sum_{k'=0}^{M-1} \Big( j(k + m_k M) Y_k e^{j \theta (k + m_k M)} \Big) \overline{\Big( j(k' + m_{k'} M) Y_{k'} e^{j \theta (k' + m_{k'} M)} \Big)} d\theta \\
&= \frac{1}{M^2} \sum_{k=0}^{M-1} \sum_{k'=0}^{M-1} (k + m_k M) \overline{(k' + m_{k'} M)} Y_k \overline{Y_{k'}} \underbrace{\frac{1}{2\pi} \int\limits_{0}^{2\pi} e^{j \theta (k + m_k M)} e^{-j \theta (k' + m_{k'} M)} d\theta}_{= \begin{cases} 0 & \text{if } k + m_k M \neq k' + m_{k'} M \\ & \iff k \neq k' \text{ for } 0 \leq k, k' < M \\ 1 & \text{if } k = k'\end{cases}} \\
&= \frac{1}{M^2} \sum_{k=0}^{M-1} |Y_k|^2 (k + m_k M)^2
\end{align*}

We now seek to minimize this by choosing $m_k$ for each $k$. Only the last term depends on $m_k$, so it's sufficient to minimize only this:

\begin{align*}
\underset{m_k}{\text{minimize}} \quad & (k + m_k M)^2 \\
\text{s.t.} \quad & 0 \leq k < M \\
	& m_k \in \mathbb{Z}
\end{align*}

This problem actually admits of good ol' calculus plus some common sense:

$$ \frac{d}{dm_k} (k + m_k M)^2 = 2(k + m_k M)M = 0 \longrightarrow m_k^* = \frac{-k}{M} \in (-1, 0] $$

where $^*$ denotes optimality. But we additionally need to choose $m_k \in \mathbb{Z}$. Let's plot it to see what's going on.

\begin{center}
\begin{tikzpicture}
	\begin{axis}[axis lines=middle,
		xlabel={$m_k$}, xlabel style={below},
		ylabel={cost},
		xmin=-1.1, xmax=0.1, xtick={-1, -0.5, 0},
		ymin=-1, ymax=4.25, yticklabels={},
		grid=both,
		width=8cm,
		height=6cm]
		\addplot[domain=-1.25:0.25, samples=100, thick] {(1 + 3*x)^2};
		\addplot[only marks, mark=+, mark size=4pt] coordinates {(-1/3,0)};
		\node at (axis cs:-1/3,0) [anchor=north] {$m_k^*$};
		\addplot[only marks, mark=*] coordinates {(-1,4)};
		\addplot[only marks, mark=*] coordinates {(0,1)};
		\node at (axis cs:-0.5,3.25) {feasible costs};
		\draw[->, bend right=30] (axis cs:-0.7, 3.25) to (axis cs:-0.95,4);
		\draw[->, bend right=30] (axis cs:-0.4, 3) to (axis cs:-0.05,1.05);
	\end{axis}
\end{tikzpicture}
\end{center}

As we change the values of $M$ and $k$, the parabola shifts around, getting taller for larger $M$ and shifting leftward as $k \rightarrow M$.

We can see that for $k \in [0, \frac{M}{2})$, the $m_k = 0$ solution is lower down the cost curve, and for $k \in (\frac{M}{2}, M)$, the $m_k = -1$ solution is more optimal. ``If $k = \frac{M}{2}$ (for even $M$), however, there is an ambiguity: either $m_k = 0$ or $-1$ gives the same value $(k + m_k M)^2 = (\frac{M}{2})^2$. For this $Y_{M/2}$ term (the ``Nyquist" term), we can arbitrarily split up the $Y_{M/2}$ term between $m = 0$ [$j\frac{M}{2}\theta$, positive frequency] and $m = -1$ [$j(\frac{M}{2} - M)\theta = -j\frac{M}{2}\theta$, negative frequency]:"

$$ Y_{M/2}(ue^{j\frac{M}{2}\theta} + (1 - u)e^{-j\frac{M}{2}\theta}) $$

where $u \in \mathbb{C}$ s.t. at sample points $\theta_n$ we get $Y_{M/2}(ue^{j\frac{M}{2}\frac{2\pi}{M}n} + (1-u)e^{-j\frac{M}{2}\frac{2\pi}{M}n}) = Y_{M/2}(u\overbrace{e^{j\pi n}}^{(-1)^n} + (1-u)\overbrace{e^{-j\pi n}}^{(-1)^n}) = Y_{M/2}(-1)^n$, which at $n = \frac{M}{2}$ will be $Y_{M/2}(-1)^{\frac{M}{2}} = Y_{M/2}\cos(\frac{M}{2}\pi)$, ``and so recover the DFT$^{-1}$."

If we use the above in the mean-squared slope derivation instead of $Y_k e^{j \theta (k + m_k M)}$ and $Y_{k'} e^{j \theta (k' + m_{k'} M)}$, then the \href{https://math.stackexchange.com/a/5013632/278341}{integral portion becomes}:

\begin{align*}
& Y_{M/2}\overline{Y_{M/2}}\frac{1}{2\pi} \int\limits_{0}^{2\pi} (ue^{j\frac{M}{2}\theta} + (1 - u)e^{-j\frac{M}{2}\theta}) \overline{(ue^{j\frac{M}{2}\theta} + (1 - u)e^{-j\frac{M}{2}\theta})} d\theta \\
&= |Y_{M/2}|^2 \frac{1}{2\pi} \Big(u\overline{u} \int\limits_{0}^{2\pi} \underbrace{e^{j\frac{M}{2}\theta} e^{-j\frac{M}{2}\theta}}_{= 1} d\theta + u \overline{(1 - u)} \int\limits_{0}^{2\pi} \underbrace{e^{j\frac{M}{2}\theta} e^{j\frac{M}{2}\theta}}_{\text{periodic!}} d\theta + (1 - u)\overline{u} \int\limits_{0}^{2\pi} \underbrace{e^{-j\frac{M}{2}\theta} e^{-j\frac{M}{2}\theta}}_{\text{periodic!}} d\theta \\
& \quad \quad + (1 - u) \overline{(1 - u)} \int\limits_{0}^{2\pi} \underbrace{e^{-j\frac{M}{2}\theta} e^{j\frac{M}{2}\theta}}_{= 1} d\theta \Big) \\
&= |Y_{M/2}|^2 \frac{1}{2\pi} (|u|^2 2\pi + |1 - u|^2 2\pi) = |Y_{M/2}|^2 (|u|^2 + |1 - u|^2)
\end{align*}

because integrating something periodic over a multiple of its period yields 0.\newline

We now know that the contribution to the mean-squared slope from the $\frac{M}{2}^{th}$ term $\propto |u|^2 + |1 - u|^2$. What's the optimal $u$?

$$\frac{d}{du} |u|^2 + |1 - u|^2 = 2u - 2(1-u) = 0 \longrightarrow u = \frac{1}{2}$$

So ``the $Y_{M/2}$ term should be \textit{equally split} between the frequencies $\pm\frac{M}{2}\theta$, giving a $\cos(\frac{M}{2}\theta)$ term." Note that if $M$ is odd, there is no troublesome $\frac{M}{2}$ term like this, but later we'll use the \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html}{Discrete Cosine Transform}\cite{dct} (DCT), which is equivalent to the FFT with even $M$ and $Y_k = Y_{M-k}$, so we do have to worry about the Nyquist term.

Now if we put it all together we get ``the \textbf{unique ``minimal-oscillation" trigonometric interpolation} of order N":

\begin{equation}\label{interpolant}
y(\theta) = \frac{1}{M} \Big(Y_0 + \sum_{0 < k < \frac{M}{2}} \big(Y_k e^{j k \theta} + Y_{M-k} e^{-j k \theta}\big) + Y_{M/2}\cos(\frac{M}{2}\theta) \Big)
\end{equation}

``As a useful side effect, this choice of trigonometric interpolation has the property that real-valued samples $y_n$ (for which $Y_0$ is real and $Y_{M-k} = \overline{Y_k}$) will result in a purely real-valued interpolation $y(\theta)$ for all $\theta$."

\subsubsection{Taking Derivatives of the Interpolant}

Now at last, with this interpolation between integer $n$ in hand, we can take a derivative w.r.t. the spatial variable:

$$\frac{d}{d\theta} y(\theta) = \frac{1}{M} \Big( \sum_{0 < k < \frac{M}{2}} j k (Y_k e^{j k \theta} + Y_{M-k} e^{-j k \theta}) - \frac{M}{2} Y_{M/2} \sin(\frac{M}{2}\theta) \Big)$$

Evaluating at $\theta_n = \frac{2\pi}{M}n, n \in \mathbb{Z}$, we get:

\begin{align*}
y'_n &= \frac{1}{M} \Big( \sum_{0 < k < \frac{M}{2}} j k (Y_k e^{j k \frac{2\pi}{M}n} + Y_{M-k} e^{-j k \frac{2\pi}{M}n}) - \overset{\text{\large 0}}{\cancel{\frac{M}{2} Y_{M/2} \sin(\pi n)}} \Big) = \frac{1}{M} \sum_{k = 0}^{M-1} Y'_k e^{j\frac{2\pi}{M}kn} \\
& \text{where} \quad Y'_k = \begin{cases} j k \cdot Y_k & k < \frac{M}{2} \\ 0 & k = \frac{M}{2} \\ j(k - M) \cdot Y_k & k > \frac{M}{2} \leftarrow \text{comes from: } k_{new} = M - k_{old}, 0 < k_{old} < \frac{M}{2} \end{cases}\\ & \hspace{5cm} \rightarrow \frac{M}{2} < k_{new} < M; -jk_{old} \cdot Y_{M - k_{old}} \rightarrow -j(M - k_{new}) \cdot Y_{k_{new}}
\end{align*}

Easy! Now let's do the second derivative:

$$\frac{d^2}{d\theta^2} y(\theta) = \frac{1}{M} \Big( \sum_{0 < k < \frac{M}{2}} (jk)^2 (Y_k e^{j k \frac{2\pi}{M}n} + Y_{M-k} e^{-j k \frac{2\pi}{M}n}) - \Big(\frac{M}{2}\Big)^2 Y_{M/2} \cos(\frac{M}{2}\theta) \Big)$$

And again evaluating at $\theta_n = \frac{2\pi}{M}n, n \in \mathbb{Z}$:

\begin{align*}
y''_n &= \frac{1}{M} \Big( \sum_{0 < k < \frac{M}{2}} (jk)^2 (Y_k e^{j k \frac{2\pi}{M}n} + Y_{M-k} e^{-j k \frac{2\pi}{M}n}) - \Big(\frac{M}{2}\Big)^2 Y_{M/2} (-1)^n \Big) = \frac{1}{M} \sum_{k = 0}^{M-1} Y''_k e^{j\frac{2\pi}{M}kn} \\
& \text{where} \quad Y''_k = \begin{cases} (j k)^2 \cdot Y_k & k < \frac{M}{2} \\ \Big(j\frac{M}{2}\Big)^2 \cdot Y_k & k = \frac{M}{2} \\ (j(k - M))^2 \cdot Y_k & k > \frac{M}{2} \end{cases} \quad \text{or equivalently} \quad Y''_k = \begin{cases} (j k)^2 \cdot Y_k & k \leq \frac{M}{2} \\ (j(k - M))^2 \cdot Y_k & k > \frac{M}{2} \end{cases}
\end{align*}

It's important to realize ``this [second derivative] procedure is \textit{not} equivalent to performing the spectral first-derivative procedure twice (unless $M$ is odd so that there is no $Y_{M/2}$ term) because the first derivative operation omits the $Y_{M/2}$ term entirely."\cite{johnson}.

We can repeat for higher derivatives, but the punchline is that for odd derivatives the $\frac{M}{2}$ term goes away, and for even derivatives it comes back. In general:

\begin{equation}\label{Y_nu}
Y^{(\nu)}_k = \begin{cases} (j k)^\nu \cdot Y_k & k < \frac{M}{2} \\ (j \frac{M}{2})^\nu \cdot Y_k & k = \frac{M}{2} \text{ and } \nu \text{ even} \\ 0 & k = \frac{M}{2} \text{ and } \nu \text{ odd} \\ (j(k - M))^\nu \cdot Y_k & k > \frac{M}{2} \end{cases}
\end{equation}

This has definite echoes of the standardly-given, continuous-time case covered in \autoref{derivative}, but it's emphatically \textit{not} as simple as just multiplying by $j\omega$ or even by $j k$. However, the final answer is thankfully super compact to represent in math and in code.

\subsection{Limitations}

So far it has all been good news, but there is a serious caveat to using the Fourier basis, especially for derivatives.

Although a Fourier transform tends to have more ``mass" at lower frequencies and fall off as we go to higher ones (otherwise the reconstruction integral would diverge), and therefore we can get really great reconstructions by leaving off higher modes\cite{kutz}, we in fact need \textit{all} the infinite modes to reconstruct the true signal\cite{oppenheim}, and even then the Fourier basis can not represent true discontinuities, instead converging ``almost everywhere", which is math speak for the ``measure" or volume of the set where it doesn't work being 0, meaning it only doesn't work \textit{at} the discontinuities themselves.\cite{oppenheim}

If there are discontinuities, we get what's called the Gibbs Phenomenon\cite{oppenheim}, essentially overshoot as the set of basis functions tries to fit a big jump. These extra wiggles are bad news for function approximation but even worse news for taking derivatives! If we end up on one of those oscillations, the slope might wildly disagree with that of the true function!

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\textwidth]{gibbs.png}
	\caption*{An example of the Gibbs phenomenon, from \cite{kutz}}
\end{figure}

This is a bigger problem than it may first appear, because when we do this on a computer, we're using the DFT, which implicitly periodically extends the function (\autoref{family}). So we not only need the function to have no jumps internal to its domain; we need it to match up \textit{at the edges} of its domain too!

This rules out the above spectral method for all but ``periodic boundary conditions"\cite{kutz}. But if the story ended right there, I wouldn't have thought it worth building this package.

\section{The Chebyshev Basis}

\href{https://www.youtube.com/watch?v=HloOBYPwlmU}{There is another} basis which we can use to represent arbitrary functions, called the \href{https://epubs.siam.org/doi/epdf/10.1137/1.9780898719598.ch8}{Chebyshev polynomials}\cite{trefethen}, which have a really neat relationship to the Fourier basis.

\begin{figure}[h!]
\begin{center}
\begin{minipage}{0.45\textwidth}
\begin{tikzpicture}
	\begin{axis}[axis lines=middle, xlabel={Re$\{z\}$}, xlabel style={xshift=12mm, yshift=-3mm},
		ylabel={Im$\{z\}$}, ylabel style={yshift=3mm}, xmin=-1.1, xmax=1.2, xtick={1},
		xticklabels={1}, xticklabel style={xshift=1mm, yshift=5mm}, ymin=-1.1, ymax=1.2, ytick=\empty,
		axis line style={->}, width=8cm, height=8cm]
		\addplot[domain=0:360,samples=100, smooth, thick] ({cos(x)}, {sin(x)}); %  Plot the unit circle
		\pgfmathsetmacro{\angle}{50}  % Angle in degrees
		\pgfmathsetmacro{\xcoord}{cos(\angle)}  % Re{z} = cos(theta)
		\pgfmathsetmacro{\ycoord}{sin(\angle)}  % Im{z} = sin(theta)
		\addplot[only marks, mark=*, mark size=2pt] coordinates {(\xcoord, \ycoord)};
		\node at (axis cs:\xcoord, \ycoord) [above right] {$z = e^{j\theta}$};
		\addplot[only marks, mark=*, mark size=2pt] coordinates {(\xcoord, -\ycoord)};
		\node at (axis cs:\xcoord, -\ycoord) [below, right, yshift=-4mm] {\parbox{15mm}{\begin{align*}\overline{z} &= z^{-1} \\ &= e^{-j\theta}\end{align*}}};
		\addplot[domain=0:1, samples=2, thick] ({x*\xcoord}, {x*\ycoord}); 
		\addplot[domain=1:0, samples=2, thick, dashed] ({\xcoord}, {x*\ycoord}); 
		\addplot[only marks, mark=*, mark size=2pt] coordinates {(\xcoord, 0)};
		\node at (axis cs:\xcoord, 0) [above right] {$x$};
		\draw [thick](axis cs:0.1,0) ++(0:1) arc (0:\angle:0.3cm);
		\node at (axis cs:0.1, 0) [above right] {$\theta$};
	\end{axis}
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.45\textwidth}
	\begin{equation}\label{domains}
	\begin{aligned}
		\text{Let} \quad & x \in [-1, 1] \phantom{\frac{0}{0}} & \text{Chebyshev} \\ & = \cos(\theta),\ \theta \in \mathbb{R} \phantom{\frac{0}{0}} & \text{Fourier} \\ & = \frac{1}{2}(z + z^{-1}),\ |z| = 1 & \text{Laurent}
	\end{aligned}
	\end{equation}
\end{minipage}
\end{center}
\end{figure}

The $k^{th}$ Chebyshev polynomial is defined as $T_k(x) = Re\{z^k\} = \frac{1}{2}(z^k + z^{-k}) = \cos(k\theta)$ by Euler formula.

\begin{align*}
T_0(x) &= Re\{z^0\} = 1\\
T_1(x) &= Re\{z^1\} = \frac{1}{2}(e^{j\theta} + e^{-j\theta}) = \cos(\theta) = x\\
T_2(x) &= \frac{1}{2}(e^{j2\theta} + e^{-j2\theta}) = \cos(2\theta)\\
& \text{but also } = \frac{1}{2}\underbrace{(z^2 + 2 + z^{-2})}_{\text{perfect square}} - 1 = \Big( \sqrt{\frac{1}{2}} (z + z^{-1}) \Big)^2 - 1 = \underbrace{\Big( \frac{2}{\sqrt{2}} \Big)^2}_{2} \underbrace{\frac{z + z^{-1}}{2}}_{\cos(\theta)} - 1 = 2x^2 - 1\\
T_3(x) &= \frac{1}{2}(e^{j3\theta} + e^{-j3\theta}) = \cos(3\theta)\\
& \text{but also } = \frac{1}{2}(z + z^{-1})^3 - \frac{3}{2}(z + z^{-1}) = 4x^3 - 3x\\
...
\end{align*}

It turns out there is a recurrent pattern:

$$ T_{k+1} = \frac{1}{2}(z^{k+1} + z^{-(k+1)}) = \frac{1}{2}(z^k + z^{-k})(z + z^{-1}) - \frac{1}{2}(z^{k-1} + z^{-(k-1)}) = 2xT_k(x) - T_{k-1}(x) $$

Due to the relationship between $\theta$ and $x$ on their respective domains, you can think of these polynomials as cosine waves ``wrapped around a cylinder and viewed from the side."\cite{trefethen}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{cylinder.png}
	\captionsetup{width=0.65\textwidth}
	\caption*{Relationship of Chebyshev domain and Fourier Domain, from \cite{cylinder}. The authors use $n$ instead of $k$, which is common for Chebyshev polynomials (e.g. \cite{trefethen}), but I prefer $k$ to enumerate basis modes, for consistency.}
\end{figure}

Essentially, on the domain $[-1, 1]$ each of these polynomials has ever more wiggles in the range $[-1, 1]$, and they perfectly coincide with the \textit{shadow} of a $2\pi$-periodic cosine in the domain $[0, \pi]$.

We can reconstruct a function using these different variables and bases, and as long as our variables are related as in \autoref{domains}, these reconstructions are \textit{equivalent}:

\begin{equation}\label{equivalent}
y(x) = \sum_{k=0}^N a_k T_k(x)\ ;\quad y(z) = \sum_{k=0}^N a_k \frac{1}{2}(z^k + z^{-k})\ ;\quad y(\theta) = \sum_{k=0}^N a_k \cos(k \theta)
\end{equation}

Note the set of ${a_k}$ is for $k \in {0, .. N}$ and therefore has cardinality $N+1$.\newline

\subsection{The Advantage of Chebyshev}

Why might we prefer this basis to the Fourier basis? Well, the advantage of a polynomial basis is we can avert the need for periodicity at the boundaries. Polynomial fits don't suffer the Gibbs Phenomenon, however they do suffer from the also-bad Runge Phenomenon\cite{kutz}:

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\textwidth]{runge.png}
	\captionsetup{width=0.6\textwidth}
	\caption*{The Runge phenomenon, demonstrated in (a) and (b), mitigated in (c) and (d), from \cite{kutz}}
\end{figure}

However, there is something we can do about the Runge phenomenon: By clustering fit points at the edges of the domain, the wild wobbles go away.

If we take $x = \cos(\theta)$ with $\theta$ equispaced $\in [0, \pi]$, then we get a very natural clustering at the edges of $[-1, 1]$ in $x$. What's more, if we have equispaced $\theta$ and a reconstruction expression built up out of sinusoids, we're back in a Fourier paradigm (at least in variable $\theta$) and can exploit the efficiency of the FFT, or, better, the discrete cosine and sine transforms!\cite{dct}\cite{dst}

\section{An Algorithm}\label{algo}

This all suggests a solution procedure:

\begin{algorithm}
\caption*{\textbf{Chebyshev Derivatives}}
\begin{algorithmic}[1] % the [1] numbers the steps, starting from 1
	\STATE Sample $y$ at $\{x_n = \cos(\theta_n)\}$ rather than at equally spaced $\{x_n\}$. This accomplishes a change of variables.
	\STATE Use the DCT to transform to frequency domain.
	\STATE Multiply by appropriate $(jk)^\nu$ to accomplish taking derivatives.
	\STATE Inverse transform using the DST if odd function, DCT if even function.
	\STATE Change variables back, taking care that the derivative in the Chebyshev domain entails an extra chain rule.
\end{algorithmic}
\end{algorithm}

There are a lot of details left to be worked out here, which we'll tackle in sequence.

\subsection{The Discrete Cosine Transform}

Because the reconstruction of $y(\theta)$ (\autoref{equivalent}) only contains cosines, doing a full FFT, which tries to fit sines as well as cosines, would be doing double work. Instead we can use the DCT. There are actually \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html}{several possible definitions of the DCT}\cite{dct}, but I use the DCT-I, because it is in some sense closest to the interpolant (\autoref{interpolant}) we've already derived.

Say

$$y = \underbrace{[\underbrace{y_0, y_1, ... y_{N-1}, y_N}_{\parbox{20mm}{\footnotesize \centering original vector, length N+1}}, \underbrace{y_{N-1}, ... y_1}_{\parbox{16mm}{\footnotesize \centering redundant information}}]}_{\text{length M = 2N, necessarily even!}}, \text{ that is: } y_n = y_{M-n},\ 0 \leq n \leq N$$

Then using $M-k$ for $k$ in the DFT equation (\autoref{dft}), we get:

\begin{align*}
Y_{M-k} &= \sum_{n=0}^{M-1} y_n e^{-j \frac{2\pi}{M}n(M-k)} = \sum_{n=0}^{M-1} y_n \underbrace{e^{-j 2\pi n}}_{1} e^{j \frac{2\pi}{M}nk} = \sum_{n = M}^{1} y_{M-n} e^{j \frac{2\pi}{M}(M - n)k} \\
&= \sum_{n=1}^{M} \underbrace{y_{M-n}}_{= y_n} \underbrace{e^{j 2\pi k}}_{1} e^{-j\frac{2\pi}{M}nk} = \sum_{n=1}^M y_n e^{-j \frac{2\pi}{M}nk} = \sum_{n=0}^{M-1} y_n e^{-j \frac{2\pi}{M}nk} = Y_k \quad \square
\end{align*}
\begin{tikzpicture}[overlay, remember picture]
	\node at (11.5,3.25) {\footnotesize let $n_{new} = M - n_{old}$};
	\node at (11,0) {\parbox{28mm}{\footnotesize because $e^{-j\frac{2\pi}{M}Mk} = e^0 = 1$ and $y_M = y_0$}};
	\draw[->] (11.5,3) -- (11.4,2.5);
	\draw[->] (10.7,0.4) -- (10.9,0.9);
\end{tikzpicture}\newline

So when $y_n$ are redundant this way, the $Y_k$ are too, in a very mirror way. We can now use the facts $Y_k = Y_{M-k}$ and $N = \frac{M}{2}$ in the FFT interpolation (\autoref{interpolant}):

\begin{equation}\label{reconstruction}
\begin{aligned}
y(\theta) &= \frac{1}{M} \Big(Y_0 + \sum_{0 < k < \frac{M}{2}} \big( Y_k e^{j k \theta} + Y_{M-k} e^{-j k \theta}\big) + Y_{M/2}\cos(\frac{M}{2}\theta) \Big) \\
&= \frac{1}{M} \Big(Y_0 + 2 \sum_{k = 1}^{N-1} \big( Y_k \underbrace{\frac{e^{j k \theta} + e^{-j k \theta}}{2}}_{\cos(k\theta)} \big) + Y_N\cos(N \theta) \Big)
\end{aligned}
\end{equation}

At samples $\theta_n = \frac{2\pi}{M}n = \frac{\pi}{N}n$, this becomes:

\[
y(\theta_n) = \frac{1}{M} \Big(Y_0 + Y_N\underbrace{\cos(\pi n)}_{(-1)^n} + 2 \sum_{k = 1}^{N-1} Y_k \cos(\frac{\pi nk}{N}) \Big) \tag{DCT$^{-1}$}
\]

This is exactly the DCT-I$^{-1}$, which, except for the $\frac{1}{M}$ term and a flip of $Y$ and $y$, is the same as the forward DCT-I! But the DCT$^{-1}$ operates on the shorter set of $Y = [Y_0, ... Y_N]$, without redundant information. Thus

\begin{equation}\label{ifftidct}
\text{FFT}^{-1}([Y_0, ... Y_N, Y_{N-1}, ... Y_1])[:N+1] = \text{DCT-I}^{-1}([Y_0, ... Y_N])
\end{equation}

Where $[:N+1]$ truncates to only the first $N$ elements. Given the equality above, we can line up everything we now know in a diagram:

\begin{figure}[h!]
\begin{center}
\begin{minipage}{0.5\textwidth}
\centering
\begin{tikzpicture}
	\node (y_long) at (0, 3) {$y_{long}$};
	\node (Y_long) at (0, 0) {$Y_{long}$};
	\node (y_short) at (4, 3) {$y_{short}$};
	\node (Y_short) at (4, 0) {$Y_{short}$};

	\draw[->, bend right=40] (y_long) to node[midway] {FFT} (Y_long);
	\draw[->, bend right=40] (Y_long) to node[midway] {FFT$^{-1}$} (y_long);
	\draw[->, bend right=40] (y_short) to node[midway] {DCT-I} (Y_short);
	\draw[->, bend right=40] (Y_short) to node[midway] {DCT-I$^{-1}$} (y_short);
	\draw[->, bend left=20] (y_long) to node[midway, above] {truncate} (y_short);
	\draw[->, bend left=20] (y_short) to node[midway, below] {truncate$^{-1}$} (y_long);
	\draw[->, bend left=20] (Y_long) to node[midway, above] {truncate} (Y_short);
	\draw[->, bend left=20] (Y_short) to node[midway, below] {truncate$^{-1}$} (Y_long);
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.25\textwidth} where truncate$^{-1}$ stacks back in redundant information\end{minipage}
\end{center}
\end{figure}

We can now easily see that in addition to the inverse relationship (\autoref{ifftidct}), we also have the forward relationship:

$$\text{FFT}([y_0, ... y_N, y_{N-1}, ... y_1])[:N+1] = \text{DCT-I}([y_0, ... y_N])$$

Before we wrap up discussion of the \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html}{DCT-I}\cite{dct}, note that the $0^{th}$ and $N^{th}$ terms appear \textit{outside the sum}, and that the sum \textit{is multiplied by 2}. In our original conception of the cosine series for $y(\theta)$ (\autoref{equivalent}), all the cosines appear equally within the sum, so our $Y_k$ are subtly different from the $a_k$ in that formulation (some scaled by a factor of $\frac{1}{2}$ and all scaled by $M$). This will be important when we later transform back to the Chebyshev domain, because we will use \textit{the DCT-I version} of $y(\theta)$, not the original conception.

\subsection{Even and Odd Derivatives and the Discrete Sine Transform}

The DCT-I can get us in to the frequency domain, but we'll need the help of another transform to get back out.

If we look at the full $Y_{long}$ with redundant information, we have a palindromic structure around $N$, but also around $0$, because of \href{https://dsp.stackexchange.com/a/18931/40873}{the repetitions}\cite{bristow}, which ensure we can read the values of $Y$ at negative $k$ by wrapping around to the end of the vector. This is describing an \textit{even} function, $f(-x) = f(x)$, which makes sense, because $y(\theta)$ is entirely composed of cosines, which are even functions, and the forward transform is symmetrical with the inverse transform, also ultimately a bunch of cosines if we were to interpolate $Y(\omega)$ from $Y_k$.

The derivative of an even function is an \textit{odd} function, $f(-x) = -f(x)$, which in principle should be constructable from purely sines, which are odd. And the derivative of an odd function is an even function again.

To see this more granularly, let's look in more detail at the multiplication by $(jk)^\nu$ that produces $Y_k^{(\nu)}$ (\autoref{Y_nu}) (the longer, not truncated version):

\begin{align*}
Y_k^{(\nu)} &= [0, j^\nu, ... (j(N-1))^\nu, \underbrace{(\color{blue}0\color{black} \text{ or } \color{red}(jN)^\nu\color{black})}_{\parbox{20mm}{\footnotesize \centering depending on $\nu$ \color{blue}odd \color{black} or \color{red}even}}, (-j(N-1))^\nu, ... (-j)^\nu] \odot Y_k\\
&= j^\nu \cdot {\underbrace{[0, 1, ... 1, (\color{blue}0 \color{black}\text{ or }\color{red} 1\color{black}), -1, ..., -1]}_{\text{\large $\tilde{\mathbbm{1}}$}}}^\nu \odot \underbrace{[0, 1, ... N-1, N, N-1, ... 1]^\nu} \odot Y_k
\end{align*}
\begin{tikzpicture}[overlay, remember picture]
	\node at (4.4,0.25) {\footnotesize constant};
	\node at (13.4,0.2) {\footnotesize palindromic};
	\draw[->] (4.4,0.5) -- (4.4,1);
	\draw[->] (12.8,0.35) -- (12,0.75);
	\draw[->] (14,0.35) -- (14.6,1);
\end{tikzpicture}\vspace{2mm}

where $\odot$ is a Hadamard, or element-wise, product, and raising a vector to a power is also element-wise. We can see

$$\tilde{\mathbbm{1}}^\nu = \begin{cases} [0, 1, ... 1, 0, -1, ..., -1] & \text{if $\nu$ is \color{blue}odd}\\ [0, 1, ... 1, 1, 1, ... 1] & \text{if $\nu$ is \color{red}even}\end{cases}$$

$[0, 1, ... 1, 0, -1, ..., -1]$ is odd around entries $0$ and $N$, and $[0, 1, ... 1, 1, 1, ... 1]$ is even around entry $0$.\newline

Let's now use this to reconstruct samples in the $\theta$ domain, $y_n$, for odd and even derivatives:

\begin{align}
y_n^{(\text{odd } \nu)} &= \frac{1}{M} \sum_{0 < k < \frac{M}{2}} (jk)^\nu (Y_k e^{jk\theta_n} \color{blue}-\color{black} \underbrace{Y_{M-k}}_{= Y_k} e^{-jk\theta_n}) = \frac{1}{M} \sum_{k=1}^{N-1} (jk)^\nu Y_k \underbrace{(e^{jk\theta_n} - e^{-jk\theta_n})}_{2j\sin(k\theta_n)} \notag \\
&= \frac{1}{M} \underbrace{2 \sum_{k=1}^{N-1} (jk)^\nu Y_k j \sin(\frac{\pi nk}{N})}_{\text{\normalsize = a DST-I of $Y_k^{(\nu)} \cdot j$!}} \label{y_prime_odd}\\
y_n^{(\text{even } \nu)} &= \frac{1}{M} \Big( \sum_{0 < k < \frac{M}{2}} (jk)^\nu (Y_k e^{jk\theta_n} \color{red}+\color{black} \underbrace{Y_{M-k}}_{= Y_k} e^{-jk\theta_n}) + (j\frac{M}{2})^\nu Y_{M/2} \cos(\frac{M}{2}\theta_n) \Big) \notag \\
&= \frac{1}{M} \Big( (jN)^\nu Y_N \cos(\pi n) + \sum_{k=1}^{N-1} (jk)^\nu Y_k \underbrace{(e^{jk\theta_n} + e^{-jk\theta_n})}_{2\cos(k\theta_n)} \Big) \notag \\
&= \frac{1}{M} \Big( \underbrace{(j0)^\nu Y_0 + (jN)^\nu Y_N (-1)^n + 2 \sum_{k=1}^{N-1} (jk)^\nu Y_k \cos(\frac{\pi nk}{N})}_{\text{\normalsize = a DCT-I of $Y_k^{(\nu)}$!}} \Big) \label{y_prime_even}
\end{align}
\begin{tikzpicture}[overlay, remember picture]
	\node at (7.2,7.25) {\parbox{13mm}{\footnotesize from oddness of $\tilde{\mathbbm{1}}^\nu$}};
	\node at (7.25,4) {\parbox{14mm}{\footnotesize from evenness of $\tilde{\mathbbm{1}}^\nu$}};
	\draw[->, blue] (7.7,7.6) -- (8,7.85);
	\draw[->, red] (7.7,4.3) -- (8.1,4.5);
\end{tikzpicture}

Brilliant! So we can use only the non-redundant $Y_k$ with a DST-I or DCT-I to convert odd and even functions, respectively, back to the $\theta$ domain!

Note that the DCT-I and DST-I definitions given in \texttt{scipy}\cite{dct}\cite{dst} use slightly different indexing than in my definitions here, which can be a point of confusion. I consistently take $N$ to be the \textit{index} of the last element of the non-redundant $y$, not its length, following \cite{trefethen}. Note too that the DST-I only takes the $k = \{1, ... N-1\}$ elements, whereas the DCT-I takes all $k = \{0, ... N\}$ elements!

\subsection{Transforming Back to the Chebyshev Domain}

At this point we've accomplished all but the last step of \hyperref[algo]{the algorithm}, but we've been operating with $y_n = y(\theta_n)$ and $y_n^{(\nu)} = y^{(\nu)}(\theta_n)$, which are really samples from the $\theta$ domain, when what we really need to do is take derivatives in the $x = \cos(\theta)$ domain.

When we do this, we have to employ a chain rule, which introduces a new factor: the derivative of one of our variables w.r.t. the other. For the $1^{st}$ derivative it looks like:

$$\frac{d}{dx} y(\theta) = \frac{d}{d\theta} y(\theta) \cdot \frac{d\theta}{dx} = \frac{\frac{d}{d\theta} y(\theta)}{\frac{dx}{d\theta}}, \quad \text{where } \frac{dx}{d\theta} = \frac{d}{d\theta} \cos(\theta) = -\sin(\theta)$$
\begin{align*}
	& \sin^2(\theta) + \cos^2(\theta) = 1 \longrightarrow \sin(\theta) = \pm \sqrt{1-\cos^2(\theta)} = \sqrt{1 - x^2} \text{ using upper semicircle}\\
	& \longrightarrow -\sin(\theta) = -\sqrt{1 - x^2} \longrightarrow \frac{d\theta}{dx} = \frac{1}{\frac{dx}{d\theta}} = \frac{-1}{\sqrt{1 - x^2}} \quad \Big(\text{and also } \frac{d}{dx} \cos^{-1}(x) = \frac{-1}{\sqrt{1 - x^2}} \Big)
\end{align*}

The $\frac{d}{d\theta} y(\theta)$ a.k.a $y'(\theta)$ term is actually pretty easy to handle, because we know its value (and for higher orders too) at discretized $\theta_n$ from earlier (\autoref{y_prime_odd} and \autoref{y_prime_even})! If we use the sampled $x_n = \cos(\theta_n)$ from step 1 of \hyperref[algo]{the algorithm}, then our $\{x_n\}$ and $\{\theta_n\}$ align, and we can find samples of the derivative w.r.t. $x$ by plugging $\{x_n\}$ in to the new factor(s) and multiplying appropriately (pointwise):

$$[\frac{d}{dx} y(\theta)]_n = \frac{-1}{\sqrt{1 - x_n^2}} \odot y'_n$$

\subsubsection{Higher Derivatives}

Let's see it for the second derivative:

\begin{equation}\label{2nd}
\begin{aligned}
\frac{d^2}{dx^2} y(\theta) &= \frac{d}{dx} \frac{y'(\theta)}{-\sqrt{1-x^2}} = \frac{-\sqrt{1-x^2} \frac{d}{dx} y'(\theta) - y'(\theta) \frac{d}{dx} (-\sqrt{1-x^2})}{1 - x^2} \\
&= \frac{-\frac{d}{d\theta}y'(\theta)\cdot \frac{d\theta}{dx}}{\sqrt{1-x^2}} - \frac{y'(\theta)\frac{x}{\sqrt{1-x^2}}}{1 - x^2} = \frac{y''(\theta)}{1-x^2} - \frac{x y'(\theta)}{(1 - x^2)^{3/2}} \\
\longrightarrow [\frac{d^2}{dx^2} y(\theta)]_n &= \frac{1}{1-x_n^2} \odot y''_n - \frac{x_n}{(1 - x_n^2)^{3/2}} \odot y'_n
\end{aligned}
\end{equation}

Notice that the $2^{nd}$ derivative in $x$ requires \textit{both} the $1^{st}$ and $2^{nd}$ derivatives in $\theta$! This pattern will turn out to hold for higher derivatives as well: For the $\nu^{th}$ derivative in $x$ we require all derivatives up to order $\nu$ in $\theta$.

Let's see a few more:

\begin{equation}\label{derivatives}
\footnotesize
\begin{aligned}
\frac{d^3}{dx^3} y(\theta) &= \frac{-1}{(1-x^2)^{3/2}} y'''(\theta) + \frac{3x}{(1-x^2)^2} y''(\theta) + \frac{-2x^2 - 1}{(1-x^2)^{5/2}} y'(\theta)\\
\frac{d^4}{dx^4} y(\theta) &= \frac{1}{(1-x^2)^2} y^{IV}(\theta) + \frac{-6x}{(1-x^2)^{5/2}} y'''(\theta) + \frac{11x^2 + 4}{(1-x^2)^3} y''(\theta) + \frac{-6x^3 - 9x}{(1-x^2)^{7/2}} y'(\theta)\\
\frac{d^5}{dx^5} y(\theta) &= \frac{-1}{(1-x^2)^{5/2}} y^V(\theta) + \frac{10x}{(1-x^2)^3} y^{IV}(\theta) + \frac{-35x^2-10}{(1-x^2)^{7/2}} y'''(\theta) + \frac{50x^2 + 55x}{(1-x^2)^4} y''(\theta) + \frac{-24x^4 -72x^2 - 9}{(1-x^2)^{9/2}} y'(\theta)\\
\end{aligned}
\normalsize
\end{equation}\vspace{2mm}

As we take higher derivatives, we can see a bit of a pattern. In particular, the function of $x$ multiplying each derivative of $y$ in $\theta$ comes from at most two terms in the preceding derivative, which have a predictable form:

\begin{align*}
\frac{d}{dx} \Big( \frac{p(x)}{(1-x^2)^{c-1}} y^{(\mu)}(\theta) &+ \frac{q(x)}{(1-x^2)^{c -\frac{1}{2}}} y^{(\mu-1)}(\theta) \Big) = \\
&\frac{p(x)}{(1-x^2)^{c-1}} y^{(\mu+1)}(\theta) \cdot \frac{d\theta}{dx} + \frac{(1 - x^2)^{c-1} \frac{d}{dx} p(x) - p(x)(c-1)(1-x^2)^{c-2}(-2x)}{(1-x^2)^{2c - 2}} y^{(\mu)}(\theta)\ +\\
&\frac{q(x)}{(1-x^2)^{c-\frac{1}{2}}} y^{(\mu)} \cdot \frac{d\theta}{dx} + \frac{(1 - x^2)^{c-\frac{1}{2}} \frac{d}{dx} q(x) - q(x)(c-\frac{1}{2})(1-x^2)^{c-\frac{3}{2}}(-2x)}{(1-x^2)^{2c - 1}} y^{(\mu-1)}(\theta)
\end{align*} 

If we now gather the $y^{(\mu)}(\theta)$ terms and use the fact $\frac{d\theta}{dx} = \frac{-1}{\sqrt{1-x^2}}$, we can find its new multiplying factor is equal to:

\begin{align*}
\frac{(1-x^2)p'(x) + 2(c-1)xp(x) - q(x)}{(1 - x^2)^c}
\end{align*}

This relationship holds no matter which, $\mu, c, p, q$ we're addressing, which allows us to build up a kind of pyramid of terms:

\begin{figure}[h!]\label{pyramid}
\begin{center}
\begin{minipage}{0.55\textwidth}
\centering
\begin{tikzpicture}
	\footnotesize
	\node[anchor=west] at (0, 4) {$-1$};
	\node[anchor=west] at (0, 3) {$-x$};\node[anchor=west] at (2.75, 3) {$1$};
	\node[anchor=west] at (0, 2) {$-2x^2-1$};\node[anchor=west] at (2.75, 2) {$3x$};\node[anchor=west] at (4.5, 2) {$-1$};
	\node[anchor=west] at (0, 1) {$-6x^3-9x$};\node[anchor=west] at (2.75, 1) {$11x^2+4$};\node[anchor=west] at (4.5, 1) {$-6x$};\node[anchor=west] at (6.5, 1) {$1$};
	\node[anchor=west] at (0, 0) {$-24x^4-72x^2-9$};\node[anchor=west] at (2.75, 0) {$50x^3+55x$};\node[anchor=west] at (4.5, 0) {$-35x^2-10$};\node[anchor=west] at (6.5, 0) {$10x$};\node[anchor=west] at (7.5, 0) {$-1$};
	\node at (0.5, -0.75) {lower $\frac{d}{d\theta}$ of y};
	\node at (8, -0.75) {higher $\frac{d}{d\theta}$ of y};
	\node at (-0.8, 2) {\parbox{15mm}{increasing $\frac{d}{dx}$ of y}};
	\node at (-1, 4) {\parbox{15mm}{\normalsize numerator $p$ and $q$:}};
	\draw[->] (0,3.8) -- (0,0.2);
	\draw[->] (0.5,-0.55) -- (0.75,-0.2);
	\draw[->] (8,-0.55) -- (7.75,-0.2);
	\draw[->] (3,1.25) -- (3,1.75) node[midway, right] {p};
	\draw[->] (2.75,1) -- (1,1.75) node[midway, above] {q};
	\node at (3, -0.75) {...}; \node at (5, -0.75) {...};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{tikzpicture}
	\node[anchor=west] at (0, 4) {$\frac{1}{2}$};
	\node[anchor=west] at (0, 3) {$\frac{3}{2}$};\node[anchor=west] at (1, 3) {$1$};
	\node[anchor=west] at (0, 2) {$\frac{5}{2}$};\node[anchor=west] at (1, 2) {$2$};\node[anchor=west] at (2, 2) {$\frac{3}{2}$};
	\node[anchor=west] at (0, 1) {$\frac{7}{2}$};\node[anchor=west] at (1, 1) {$3$};\node[anchor=west] at (2, 1) {$\frac{5}{2}$};\node[anchor=west] at (3, 1) {$2$};
	\node[anchor=west] at (0, 0) {$\frac{9}{2}$};\node[anchor=west] at (1, 0) {$4$};\node[anchor=west] at (2, 0) {$\frac{7}{2}$};\node[anchor=west] at (3, 0) {$3$};\node[anchor=west] at (4, 0) {$\frac{5}{2}$};
	\node at (1, -0.75) {...};\node at (4, -0.75) {...};
	\node at (-1, 4) {\parbox{15mm}{\normalsize $c$ at each location:}};
\end{tikzpicture}
\end{minipage}
\end{center}
\end{figure}\vspace{-5mm}

$q$ always refers to the element up and to the left, and $p$ always refers to the element above. If the arrows roll out of the pyramid, the corresponding $p$ or $q$ is $0$.

I've done the above programmatically in \href{https://github.com/pavelkomarov/spectral-derivatives/blob/main/specderiv/specderiv.py}{the code}, such that we can find and apply the factors---and thereby accomplish the variable transformation back to the Chebyshev domain---for arbitrarily high derivatives. You're welcome.

\subsubsection{Handling Domain Endpoints}\label{endpoints}

There's a problem in the above at the edges of the domain: If $x = \pm 1$, the denominators of all the factors, which are powers of $\sqrt{1 - x^2} = 0$!

However, this doesn't mean $\frac{d^\nu}{dx^\nu} y$ can't have a valid \textit{limit value} at those points. First remember our reconstruction $y(\theta)$ is composed of cosines, so notice if we take odd derivatives in $\theta$, we get sines, and at the edges of the domain where $x \rightarrow \pm 1$, $\theta \rightarrow 0,\pi$, sine will be 0! However, if we take even derivatives, then $\cos(0,\pi) \rightarrow 1, -1$. Then, if we look closely at the derivatives in $x$ (\autoref{derivatives}), we can see that \textit{even} derivatives in $\theta$ of $y$ are divided by \textit{even} powers of $\sqrt{1 - x^2}$, and the \textit{highest} power in a denominator is an \textit{odd} power of $\sqrt{1 - x^2}$. If we multiply through so everything is over the highest-power denominator and then combine the expression into a single fraction, we get a situation where the odd-derivative terms are 0 because sines, and the even-derivative terms are 0 because they're multiplied by at least one $\sqrt{1 - x^2}$.

This means the \textit{numerator} as well as the denominator is $0$ at the domain endpoints. $\frac{0}{0}$ is an indeterminate form, so we can use L'HÃ´pital's rule!

Let's see it for the $1^{st}$ derivative. Plugging in the DCT-I reconstruction (\autoref{reconstruction}) for $y(\theta)$:

\begin{align*}
&\lim_{\substack{x \to \pm 1 \\ \theta \to 0,\pi}} \frac{d}{d\theta} \frac{1}{M} \Big( Y_0 + Y_N \cos(N\theta) + 2 \sum_{k=1}^{N-1} Y_k \cos(k\theta) \Big) \cdot \frac{d\theta}{dx} = \lim_{\substack{x \to \pm 1 \\ \theta \to 0,\pi}} \frac{\frac{1}{M} \Big(-N Y_N \sin(N\theta) - 2 \sum_{k=1}^{N-1} k Y_k \sin(k\theta) \Big)}{-\sqrt{1-x^2}} \\
&\underset{\frac{d}{dx}}{\overset{\frac{d}{dx}}{\longrightarrow}}\ = \lim_{\substack{x \to \pm 1 \\ \theta \to 0,\pi}} \frac{\frac{1}{M} \Big(-N^2 Y_N \cos(N\theta) - 2 \sum_{k=1}^{N-1} k^2 Y_k \cos(k\theta) \Big) \cdot \frac{-1}{\cancel{\sqrt{1-x^2}}}}{\frac{x}{\cancel{\sqrt{1-x^2}}}} = \lim_{\substack{x \to \pm 1 \\ \theta \to 0,\pi}} \frac{\frac{1}{M} \Big(N^2 Y_N \cos(N\theta) + 2 \sum_{k=1}^{N-1} k^2 Y_k \cos(k\theta) \Big)}{x}
\end{align*}

\[
= \begin{cases} \frac{1}{M} \Big(N^2 Y_N + 2 \sum_{k=1}^{N-1} k^2 Y_k \Big) & \text{ at } x=1, \theta=0 \\ -\frac{1}{M} \Big(N^2 (-1)^N Y_N + 2 \sum_{k=1}^{N-1} k^2 (-1)^k Y_k \Big) & \text{ at } x=-1, \theta=\pi \end{cases} \tag{$1^{st}$ endpoints}
\]\vspace{2mm}

And now let's do it for the $2^{nd}$ derivative, with some slightly more compact notation:

\begin{align*}
&\lim_{\substack{x \to \pm 1 \\ \theta \to 0,\pi}} \frac{\sqrt{1 - x^2} y''(\theta) - x y'(\theta)}{(1-x^2)^{3/2}} \rightarrow \frac{0}{0} \underset{\frac{d}{dx}}{\overset{\frac{d}{dx}}{\longrightarrow}} \frac{\cancel{\sqrt{1-x^2}}y'''(\theta)\frac{-1}{\cancel{\sqrt{1-x^2}}} + \cancel{\frac{-x}{\sqrt{1-x^2}}y''(\theta)} - (\cancel{xy''(\theta)\frac{-1}{\sqrt{1-x^2}}} + y'(\theta))}{-3x\sqrt{1-x^2}}\\
& \rightarrow \frac{0}{0} \underset{\frac{d}{dx}}{\overset{\frac{d}{dx}}{\longrightarrow}} \frac{\cancel{-}y^{IV}(\theta)\frac{\cancel{-1}}{\cancel{\sqrt{1-x^2}}} \cancel{-} y''(\theta)\frac{\cancel{-1}}{\cancel{\sqrt{1-x^2}}}}{\frac{6x^2 - 3}{\cancel{\sqrt{1-x^2}}}} = \frac{1}{6x^2 - 3}(y^{IV}(\theta) + y''(\theta))
\end{align*}

We already know $y''(\theta) = \frac{1}{M} \Big(-N^2 Y_N \cos(N\theta) - 2 \sum_{k=1}^{N-1} k^2 Y_k \cos(k\theta) \Big)$. We can easily find

$$y^{IV}(\theta) = \frac{1}{M} \Big(N^4 Y_N \cos(N\theta) + 2 \sum_{k=1}^{N-1} k^4 Y_k \cos(k\theta) \Big)$$

Now we can evaluate these and the factor $\frac{1}{6x^2 - 3}$ at the limit values and put it all together to find: 

\[
\begin{cases} \frac{1}{3M} \Big((N^4 - N^2) Y_N + 2 \sum_{k=1}^{N-1} (k^4 - k^2) Y_k \Big) & \text{ at } x=1, \theta=0 \\ \frac{1}{3M} \Big((N^4 - N^2)(-1)^N Y_N + 2 \sum_{k=1}^{N-1} (k^4 - k^2) (-1)^k Y_k \Big) & \text{ at } x=-1, \theta=\pi \end{cases} \tag{$2^{nd}$ endpoints}
\]\vspace{2mm}

It's possible to do this for higher derivatives too. I actually did it for the $3^{rd}$ and $4^{th}$ derivatives and put the answers in \href{https://github.com/pavelkomarov/spectral-derivatives/blob/main/specderiv/specderiv.py}{the code}. However, in general to find the endpoints for the $\nu^{th}$ derivative takes $\nu$ applications of L'HÃ´pital's rule, and the algebra gets to be pretty gnarly.

In principle it is possible to build up the formulas via a similar process to the \hyperref[pyramid]{pyramid scheme} from earlier, because $f(x)$ factors multiplying respective orders of $y^{(\mu)}(\theta)$ come from at most two terms in the previous expression (before the latest application of L'HÃ´pital). But this time the $f(x)$s involve fractional powers of $\sqrt{1-x^2}$, which can't be as easily represented in \texttt{numpy}. So for now a neat scheme \href{https://github.com/pavelkomarov/spectral-derivatives/issues/1}{hasn't been invented}.

It is possible to sidestep these ballooningly-complicated variable mappings and higher derivative limit-evaluations by building up to higher derivatives from lower ones in stages (being careful to only use even derivatives if our end goal is even, so we preserve the Nyquist term). This would involve more forward transforms but actually no more inverse ones, since we have to remember the results of all the inverse transforms up to the $\nu^{th}$ order anyway.

\section{Multidimensionality}

We are now fully equipped to find derivatives for 1-dimensional data. This is technically all we need, because, due to linearity of the derivative operator, we can find the derivative along a particular dimension of a multidimensional space by using our 1D solution along each constituent vector running in that direction, and we can find derivatives along multiple dimensions by applying the above in series along each dimension:

$$\frac{\partial^2}{\partial x_1 \partial x_2} y(x_1, x_2) = \texttt{Algo}(\texttt{Algo}(y_i, 1^{st}, x_1)_j, 1^{st}, x_2) \quad \forall\ i, j$$

$$ \nabla^2 y = (\frac{\partial^2}{\partial x_1^2} + \frac{\partial^2}{\partial x_2^2}) y = \texttt{Algo}(y_i, 2^{nd}, x_1) + \texttt{Algo}(y_j, 2^{nd}, x_2) \quad \forall\ i, j$$

where $i, j$ are indexers as in the computing sense and have nothing to do with the imaginary unit, \texttt{Algo} applies \hyperref[algo]{the algorithm} to each vector along the dimension given by the third argument, and the $1^{st}$ and $2^{nd}$ in the second argument refer to the derivative order.

Each application to a vector incurs $O(N \log N)$ cost, and fundamentally applying the method to higher-dimensional data must involve a loop, so the full cost of applying along any given direction is (assuming length $N$ in all dimensions) $O(N^D \log N)$, where $D$ is the dimension of the data. Aside from pushing this loop lower down into \texttt{numpy} to take advantage of vectorized compute, there can be no cost savings for a derivative in a particular dimension.

\subsection{Dimensions Together versus In Series}

But can we simplify the situation at all?

Due to the linearity of the Fourier transform, transforming along all dimensions, multiplying by appropriate $(jk)^\nu$ along corresponding dimensions of the transformed data, and then inverse transforming along all dimensions is equivalent to transforming, multiplying, and inverse transforming each dimension in series\cite{johnson}:

\begin{center}
\begin{tikzpicture}
	\node[draw, rectangle, minimum width=3cm, minimum height=3cm] (y) {$y$};
	\node[draw, rectangle, minimum width=3cm, minimum height=3cm, right=of y] (Y) {$Y$};
	\node[draw, rectangle, minimum width=3cm, minimum height=3cm, right=of Y] (jkY) {$(jk)^{\tilde{\nu}} \odot Y$};
	\node[draw, rectangle, minimum width=3cm, minimum height=3cm, right=of jkY] (dy) {$dy$};
	\draw[->] (y) -- (Y);
	\draw[->] (Y) -- (jkY);
	\draw[->] (jkY) -- (dy);
	\draw[->] (-1,1.75) -- (-1,-1.75) node[midway, above left, yshift=3mm, xshift=1mm] {\footnotesize FFT};
	\draw[->] (-1.75,-1) -- (1.75,-1) node[midway, above] {\footnotesize FFT};
	\draw[<->] (3,1.75) -- (3,-1.75) node[midway, above left, yshift=3mm, xshift=1mm] {\footnotesize $\odot (jk)^{\nu_d}$};
	\draw[<->] (2.25,-1) -- (5.75,-1) node[midway, above] {\footnotesize $\odot (jk)^{\nu_{d'}}$};
	\draw[<-] (7,1.75) -- (7,-1.75) node[midway, above left, yshift=3mm, xshift=1mm] {\footnotesize FFT$^{-1}$};
	\draw[<-] (6.25,-1) -- (9.75,-1) node[midway, above] {\footnotesize FFT$^{-1}$};
	\node[anchor=north, yshift=-2mm] at (y.south) {$O(D \cdot N^D \log N)$};
	\node[anchor=north, yshift=-2mm] at (Y.south) {$O(D \cdot N^D)$};
	\node[anchor=north, yshift=-2mm] at (jkY.south) {$O(D \cdot N^D \log N)$};
\end{tikzpicture}
\end{center}

That's really neat, but does it save us anything, really? Let's see it in series:

\begin{center}
\begin{tikzpicture}
	\node[draw, rectangle, minimum width=3cm, minimum height=3cm] (y) {$y$};
	\node[draw, rectangle, minimum width=3cm, minimum height=3cm, right=of y] (Y1) {$Y_d$};
	\node[draw, rectangle, minimum width=3cm, minimum height=3cm, right=of Y] (jkY1) {$(jk)^{\nu_d} \odot Y_d$};
	\node[draw, rectangle, minimum width=3cm, minimum height=3cm, right=of jkY] (dy1) {$\partial_d y$};
	\node[draw, rectangle, minimum width=3cm, minimum height=3cm, below=of dy1] (Y2) {$Y_{d'}$};
	\node[draw, rectangle, minimum width=3cm, minimum height=3cm, left=of Y2] (jkY2) {$(jk)^{\nu_{d'}} \odot Y_{d'}$};
	\node[draw, rectangle, minimum width=3cm, minimum height=3cm, left=of jkY2] (dy2) {$\partial_{d'} \partial_{d} y$};
	\node[left=of dy2, font=\huge] (dots) {...};
	\draw[->] (y) -- (Y);
	\draw[->] (Y) -- (jkY);
	\draw[->] (jkY) -- (dy1);
	\draw[->] (dy1) -- (Y2);
	\draw[->] (Y2) -- (jkY2);
	\draw[->] (jkY2) -- (dy2);
	\draw[->] (dy2) -- (dots);
	\draw[->] (11,1.75) -- (11,-1.75) node[midway, above left, yshift=3mm, xshift=1mm] {\footnotesize FFT};
	\draw[->] (-1.75,-1) -- (1.75,-1) node[midway, above] {\footnotesize FFT};
	\draw[<->] (11,-2.25) -- (11,-5.75) node[midway, above left, yshift=3mm, xshift=1mm] {\footnotesize $\odot (jk)^{\nu_{d'}}$};
	\draw[<->] (2.25,-1) -- (5.75,-1) node[midway, above] {\footnotesize $\odot (jk)^{\nu_d}$};
	\draw[<-] (7,-2.25) -- (7,-5.75) node[midway, above left, yshift=3mm, xshift=1mm] {\footnotesize FFT$^{-1}$};
	\draw[<-] (6.25,-1) -- (9.75,-1) node[midway, above] {\footnotesize FFT$^{-1}$};
	\node[anchor=north] at (y.south) {$O(N^D \log N)$};
	\node[anchor=north] at (Y.south) {$O(N^D)$};
	\node[anchor=north] at (jkY.south) {$O(N^D \log N)$};
	\node at (0, -4.75) {\parbox{19mm}{repeat for $D$ dimensions}};
\end{tikzpicture}
\end{center}

If we add up the costs, we can see that it's actually no more or less efficient to differentiate along all dimensions at once versus in series.

\subsection{Splintering}

The above is good news, because converting back to the Chebyshev domain turns out to be terribly knotty and difficult in the simultaneous-multidimensional derivatives case, for several reasons.

First, the fact we need all inverse transforms in $\theta$ up to order $\nu$ to compute the derivative of order $\nu$ in $x$ leads to \textit{far} more terms in multiple dimensions, due to interactions. I'll demonstrate with the simplest possible case, $2^{nd}$ order in one dimension and $1^{st}$ order in another:

\begin{equation}\label{minexample}
\frac{\partial^3}{\partial x_1^2 \partial x_2} y(\theta_1, \theta_2) = \frac{\partial}{\partial x_2} \Big[ \frac{\partial^2}{\partial x_1^2} y(\theta_1, \theta_2) \Big]
\end{equation}

Let's break this up and apply the \href{https://math.libretexts.org/Bookshelves/Calculus/Calculus_(OpenStax)/14%3A_Differentiation_of_Functions_of_Several_Variables/14.05%3A_The_Chain_Rule_for_Multivariable_Functions}{multivariable chain rule}\cite{chainrule} and product rule to just evaluate that inner portion first:

\begin{align*}
\frac{\partial^2}{\partial x_1^2} y(\theta_1, \theta_2) &= \frac{\partial}{\partial x_1} \Big( \frac{\partial}{\partial \theta_1} y \cdot \frac{d\theta_1}{dx_1} + \frac{\partial}{\partial \theta_2} y \cdot \overset{\text{\normalsize 0}}{\cancel{\frac{d\theta_2}{dx_1}}} \Big) & \text{multivariable chain rule}\\
&= \frac{\partial}{\partial \theta_1} y \cdot \frac{d^2 \theta_1}{dx_1^2} + \frac{\partial}{\partial x_1}\Big( \frac{\partial}{\partial \theta_1} y \Big) \cdot \frac{d\theta_1}{d x_1} & \text{product rule}\\
&= \frac{\partial}{\partial \theta_1} y \cdot \frac{d^2 \theta_1}{dx_1^2} + \Big( \frac{\partial^2}{\partial \theta_1^2} y \cdot \frac{d \theta_1}{dx_1} + \frac{\partial^2}{\partial \theta_2 \partial \theta_1} y \overset{\text{\normalsize 0}}{\cancel{\frac{d\theta_2}{dx_1}}} \Big) \cdot \frac{d\theta_1}{d x_1} & \text{multivariable chain rule}\\
&= \frac{\partial}{\partial \theta_1} y \cdot \frac{d^2 \theta_1}{dx_1^2} + \frac{\partial^2}{\partial \theta_1^2} y \cdot \Big( \frac{d \theta_1}{dx_1} \Big)^2
\end{align*}

Now using this to evaluate \autoref{minexample}:

\begin{align*}
\frac{\partial^3}{\partial x_1^2 \partial x_2} y(\theta_1, \theta_2) =& \frac{\partial}{\partial x_2} \Big[ \frac{\partial}{\partial \theta_1} y \cdot \frac{d^2 \theta_1}{dx_1^2} + \frac{\partial^2}{\partial \theta_1^2} y \cdot \Big( \frac{d \theta_1}{dx_1} \Big)^2 \Big]\\
=& \frac{\partial}{\partial \theta_1} y \cdot \overset{\text{\normalsize 0}}{\cancel{\frac{\partial}{\partial x_2} \frac{d^2 \theta_1}{dx_1^2}}} + \Big( \frac{\partial^2}{\partial \theta_1^2} y \cdot \overset{\text{\normalsize 0}}{\cancel{\frac{d\theta_1}{dx_2}}} + \frac{\partial^2}{\partial \theta_1 \partial \theta_2} y \cdot \frac{d \theta_2}{dx_2} \Big) \cdot \frac{d^2\theta_1}{dx_1^2} \\
&+ \frac{\partial^2}{\partial \theta_1^2} y \overset{\text{\normalsize 0}}{\cancel{\frac{\partial}{\partial x_2} \Big( \frac{d\theta_1}{dx_1} \Big)^2}} + \Big( \frac{\partial^3}{\partial \theta_1^3} y \cdot \overset{\text{\normalsize 0}}{\cancel{\frac{d\theta_1}{dx_2}}} + \frac{\partial^3}{\partial \theta_1^2 \partial \theta_2} y \cdot \frac{d \theta_2}{dx_2} \Big) \cdot \Big(\frac{d\theta_1}{dx_1}\Big)^2 \\
=& \frac{\partial^2}{\partial \theta_1 \partial \theta_2} y \cdot \frac{d \theta_2}{dx_2} \cdot \frac{d^2\theta_1}{dx_1^2} + \frac{\partial^3}{\partial \theta_1^2 \partial \theta_2} y \cdot \frac{d \theta_2}{dx_2} \cdot \Big(\frac{d\theta_1}{dx_1}\Big)^2
\end{align*}

The double derivative in $x_1$ has \textit{splintered} the expression into two, and then the single derivative in $x_2$ has interacted with \textit{both} those terms.

All the terms that involve derivatives of a $\theta$ w.r.t. an $x$ are ultimately just functions of $x$. In fact, $\frac{d\theta_2}{dx_2}$ is just our old friend $\frac{-1}{\sqrt{1-x_2^2}}$, and you can pick out $\big( \frac{d\theta_1}{dx_1} \big)^2$ and $\frac{d^2\theta_1}{dx_1^2}$ in \autoref{2nd}. Together they are \textit{a Cartesian product} of the \hyperref[pyramid]{the 1D case}!

Meanwhile, the $\partial y$ terms have different orders, which means that to find them we need to multiply $Y$, the all-dimensions transform of $y$, by \textit{different} orders of $(jk)$. If we do this carefully, the best-case scenario is that we incur \href{https://github.com/pavelkomarov/spectral-derivatives/issues/2}{the same amount of work} as the in-series case, but it takes some extra bookkeeping and data copying.

Most gnarly, at the \textit{edges} of the domain we still need to use L'HÃ´pital's rule on an analytic expression to evaluate the limits of

$$\frac{\partial^{\sum\nu_i}}{\partial x_1^{\nu_1} ... \partial x_D^{\nu_D}} y(\theta_1, ... \theta_D)$$

This is made worse by the fact our analytic expression is based on the DCT-I, which has terms outside the central sum, so as we substitute the DCT-I in to the DCT-I, we get ever more terms ($3^D$ of them), e.g. in 2D:

\begin{align*}
y(\theta_1, \theta_2) =& \frac{1}{M^2} \Big[ Y_{00} + Y_{N0}\cos(N\theta_1) + Y_{0N}\cos(N\theta_2) + Y_{NN}\cos(N\theta_1)\cos(N\theta_2)\\
&+ 2\sum_{k_1 = 1}^{N-1} Y_{k_1 0} \cos(k_1 \theta_1) + 2\sum_{k_2 = 1}^{N-1} Y_{0 k_2} \cos(k_2 \theta_2) + 2\cos(N\theta_2) \sum_{k_1 = 1}^{N-1} Y_{k_1 N} \cos(k_1 \theta_1)\\
&+ 2\cos(N\theta_1) \sum_{k_2 = 1}^{N-1} Y_{N k_2} \cos(k_2 \theta_2) + 4 \sum_{k_1 = 1}^{N-1} \sum_{k_2 = 1}^{N-1} Y_{k_1 k_2} \cos(k_1 \theta_1) \cos(k_2 \theta_2)\Big]
\end{align*}

We could generalize the original conception (\autoref{equivalent}), which has a single sum with $a_0 = \frac{Y_0}{M}, a_k = \frac{2 Y_k}{M} \text{ for } k \in [1, N-1], a_N = \frac{Y_N}{M}$, to get a $y(\vec{\theta})$ with only a single term, but this involves still more extra bookkeeping.

Worst of all, we then still have to take limits as different \textit{combinations} of dimensions reach the edges, which becomes a combinatorial nightmare. This was \hyperref[endpoints]{already hard enough in 1D}!

So although \texttt{numpy} does provide the \texttt{fftn} function for transforming in multiple dimensions at once, and \texttt{scipy} provides similar \texttt{dctn} and \texttt{dstn} functions, they wouldn't confer a computational-complexity benefit and would require the math and code to get massively more complicated, so I have chosen not to use them. From a user-friendliness perspective, I also judge it to be somewhat more confusing to specify multiple derivatives at once (although generalizing the \texttt{nu} and \texttt{axis} parameters to vectors is possible). For all these reasons, I have chosen to limit the package to taking derivatives along a single dimension at once. Multidimensional \textit{data} can still be handled, however, via clever indexing and use of \texttt{fft}, \texttt{dct}, and \texttt{dst}'s \texttt{axis} parameter.

\printendnotes

\begin{thebibliography}{99} % https://tex.stackexchange.com/questions/198330/argument-in-thebibliography
\raggedright 
\bibitem{lebesgue}
	Lebesgue Integrable, https://mathworld.wolfram.com/LebesgueIntegrable.html
\bibitem{hamming}
	Hamming, R., \textit{The Art of Doing Science and Engineering}
\bibitem{taylor}
	Pego, B., Simplest proof of Taylor's theorem, https://math.stackexchange.com/a/492165/278341
\bibitem{complex}
	Disintegration By Parts, Why do Fourier transforms use complex numbers?, https://math.stackexchange.com/a/1293127/278341
\bibitem{michigan}
	Yagle, A., 2005, https://web.eecs.umich.edu/~aey/eecs206/lectures/fourier2.pdf
\bibitem{why}
	https://math.stackexchange.com/questions/1105265/why-do-fourier-series-work
\bibitem{swarthmore}
	Derivation of Fourier Series, http://lpsa.swarthmore.edu/Fourier/Series/DerFS.html
\bibitem{sego}
	Sego, D., Demystifying Fourier analysis, https://dsego.github.io/demystifying-fourier/
\bibitem{medium}
	Nakagome, S. Fourier Transform 101 â€” Part 4: Discrete Fourier Transform, https://medium.com/sho-jp/fourier-transform-101-part-4-discrete-fourier-transform-8fc3fbb763f3
\bibitem{oppenheim}
	Oppenheim, A. \& Willsky, A., 1996, \textit{Signals and Systems, 2nd Ed.}
\bibitem{brunton}
	Brunton, S., The Fourier Transform and Derivatives, https://www.youtube.com/watch?v=d5d0ORQHNYs
\bibitem{kutz}
	Kutz, J.N., 2023, \textit{Data-Driven Modeling \& Scientific Computation}, Ch. 11, https://faculty.washington.edu/kutz/kutz\_book\_v2.pdf
\bibitem{numpy}
	Discrete Fourier Transform, https://numpy.org/doc/2.1/reference/routines.fft.html
\bibitem{bretherton}
	The Fourier spectral method, https://atmos.washington.edu/\%7Ebreth/classes/AM585/lect/DFT\_FS\_585\_notes.pdf
\bibitem{bristow}
	Bristow-Johnson, R., 2014, About Discrete Fourier Transform vs. Discrete Fourier Series, https://dsp.stackexchange.com/a/18931/40873
\bibitem{johnson}
	Johnson, S., 2011, Notes on FFT-based differentiation, https://math.mit.edu/\%7Estevenj/fft-deriv.pdf
\bibitem{dct}
	https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html
\bibitem{dst}
	https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dst.html
\bibitem{trefethen}
	Trefethen, N., 2000, \textit{Spectral Methods in Matlab}, https://epubs.siam.org/doi/epdf/10.1137/1.9780898719598.ch8
\bibitem{cylinder}
	https://www.researchgate.net/figure/Chebyshev-polynomials-a-can-be-viewed-as-cosine-functions-b-drawn-on-a-cylinder-and\_fig1\_340905766
\bibitem{chainrule}
	The Chain Rule for Multivariable Functions, https://math.libretexts.org/Bookshelves/Calculus/Calculus\newline\_(OpenStax)/14\%3A\_Differentiation\_of\_Functions\_of\_Several\_Variables/14.05\%3A\_The\_Chain\_Rule\_for\newline\_Multivariable\_Functions
\end{thebibliography}

\end{document}